<html>
<head>
<title>TREC Dynamic Domain in AWS Public Data Sets</title>
</head>
</body>
<H1>TREC Dynamic Domain in AWS Public Data Sets</H1>
<p>Please see <a href="http://trec-dd.org/">trec-dd.org</a> for details.

<p>This bucket hosts data sets for TREC, and this subdirectory holds data sets for TREC DD:
<ul>
 <li> <a href="#illicitgoods">Illicit Goods Forums</a></li>
 <li> <a href="#ebola">Ebola</a></li>
 <li> <a href="#localpolitics">Local Politics</a></li>
 <li> <a href="#polar">Polar Sciences</a></li>
</ul>

<h3> Illicit Goods <a name="illicitgoods"/></h3>
<p>This corpus contains crawl results from two illicit goods forums, Hackforums.net and Blackhatworld.com. Specifically, we crawled several subforums and threads that were identified as being "most interesting" and where most of the transactional activity takes place.</p>
<p>The Hackforums dataset contains 2,116,563 items, where items represent posts on various forum threads. Also, for posts where an image was detected, we ran the image through OCR. The dataset can be downloaded from data.hyperiongray.com either as a single .gz file (686MB compressed, 3.3GB decompressed) or as 14 smaller files (50MB each). Send an email to atowler@hyperiongray.com for credentials to the data server.</p>
<p>The Blackhatworld dataset contains 3,961,388 items. It can also be downloaded at data.hyperiongray.com, in 279 files of 50MB each.</p>

<h3> Ebola <a name="ebola"/></h3>
<p>coming soon...</p>

<h3> Polar <a name="polar"/></h3>
<p>This dataset contains crawl results from three polar data repositories: the National Science Foundation Advanced Cooperative Arctic Data and Information System (ACADIS), the National Aeronautics and Space Administration Antarctic Master Directory (AMD) and the National Snow and Ice Data Center (NSIDC) Arctic Data Explorer (ADE). </p>
<p>The Polar dataset contains 1,741,530 items, which represent a rich combination of web documents, PDF documents, scientific data files (HDF, NetCDF, Gridded Binary, etc.) collected over space (world; regional; Arctic; Antarctic) and time (years-decades), with rich metadata features, and content. For credentials to access this dataset (158GB) email chris.a.mattmann@nasa.jpl.gov.</p>

<h3>Local Politics <a name="localpolitics"/></h3>

<p>This particular corpus is a selection of the TREC KBA 2014
StreamCorpus that has already been tagged with Serif NER, and is
organized into hourly directories based on the origination time stamp
on each document.

<p>This subcorpus for TREC DD has 6,831,451 files that are stored in
816,879 chunk files here in S3.  It is approximately 247GB of XZ
compressed files.  The example script linked below shows how to
iterate over all of them to get the cleansed html, or cleansed visible
text, or the NER output.  Note that this data has also
been <a href="http://trec-kba.org/data/fakba1/">annotated by Google's
internal entity linking tools to provide references to
Freebase.</a></p>

<p>The full list of files is available at
<a href="https://aws-publicdatasets.s3.amazonaws.com/trec/dd/local-politics-streamcorpus-v0_3_0-s3-paths.txt.xz">local-politics-streamcorpus-v0_3_0-s3-paths.txt.xz</a>
and that file must be downloaded to your local directory in order for
<a href="https://aws-publicdatasets.s3.amazonaws.com/trec/dd/local-politics-read-example-script.py">this example script to work</a>.  To run the example script, you can:
<pre>
sudo apt-get install python-virtualenv liblzma-dev python-dev
virtualenv ve
source ve/bin/activate
pip install requests backports.lzma streamcorpus
</pre>
</p>

<p>Installation on CentOS/RHEL is similar using yum instead of apt-get</p>

<p>This script also requires that <a href="http://trec.nist.gov/data/kba.html">you get the GPG decryption key from NIST</a>.</p>

<p>The filtering process that generated this data set used these
substrings:
<a href="https://aws-publicdatasets.s3.amazonaws.com/trec/dd/local-politics-streamcorpus-pipeline-filter-domains.txt.xz.gpg">local-politics-streamcorpus-pipeline-filter-domains.txt.xz.gpg</a>
and this streamcorpus_pipeline configuration file:
<a href="https://aws-publicdatasets.s3.amazonaws.com/trec/dd/local-politics-streamcorpus-pipeline-filter-config.yaml">local-politics-streamcorpus-pipeline-filter-config.yaml</a>

and this command:
<pre>streamcorpus_pipeline -c local-politics-streamcorpus-pipeline-filter-config.yaml -i path_to_input_S3_file</pre>
which you can install with `pip install streamcorpus-pipeline`

</body>
</html>

